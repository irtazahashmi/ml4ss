{"cells":[{"cell_type":"markdown","metadata":{"id":"TvWTPz5pUYqD"},"source":["# TPM034A Machine Learning for socio-technical systems\n","## `Lab session 03:  Working with image embeddings`\n","\n","**Delft University of Technology**<br>\n","**Q2 2023**<br>\n","**Instructor:** Sander van Cranenburgh <br>\n","**TAs:**  Francisco Garrido Valenzuela & Lucas Spierenburg <br>"]},{"cell_type":"markdown","metadata":{"id":"TZ1AZUb-UYqG"},"source":["### `Instructions`\n","\n","**Lab sessions aim to:**<br>\n","* Show and reinforce how models and ideas presented in class are used in practice.<br>\n","* Help you gather hands-on machine learning skills.<br>\n","\n","**Lab sessions are:**<br>\n","* Learning environments where you work with Jupyter notebooks and where you can get support from TAs and fellow students.<br>\n","* Not graded and do not have to be submitted."]},{"cell_type":"markdown","metadata":{"id":"1TJg6JUXUYqH"},"source":["### `Use of AI tools`\n","AI tools, such as ChatGPT and Co-pilot, are great tools to assist with programming. Moreover, in your later careers you will work in a world where such tools are widely available. As such, we **encourage** you to use AI tools **effectively** (both in the lab sessions and assignments). However, be careful not to overestimate the capacity of AI tools! AI tools cannot replace you: you still have to conceptualise the problem, dissect it and structure it, to conduct proper analysis and modelling. We recommend being especially **reticent** with using AI tools for the more conceptual and reflective oriented questions."]},{"cell_type":"markdown","metadata":{"id":"o6rctP7CUYqH"},"source":["### `Workspace set-up`\n","\n","**Option 1: Local environment**<br>\n","Uncomment the following cell if you are running this notebook on your local environment. This will install all dependencies on your Python version."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y-yXyXTDUYqI"},"outputs":[],"source":["#!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"HKhSp7MoUYqJ"},"source":["**Option 2: Google Colab**<br>\n","Uncomment the following cells code lines if you are running this notebook on Colab"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T1DHYwZSUYqJ","executionInfo":{"status":"ok","timestamp":1701768768773,"user_tz":-120,"elapsed":34734,"user":{"displayName":"Irtaza Hashmi","userId":"15975256869036552807"}},"outputId":"20e2b8d0-4848-40a6-9eeb-797a577c3fea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Q2_2023'...\n","remote: Enumerating objects: 218, done.\u001b[K\n","remote: Counting objects: 100% (59/59), done.\u001b[K\n","remote: Compressing objects: 100% (29/29), done.\u001b[K\n","remote: Total 218 (delta 28), reused 53 (delta 27), pack-reused 159\u001b[K\n","Receiving objects: 100% (218/218), 94.70 MiB | 12.59 MiB/s, done.\n","Resolving deltas: 100% (74/74), done.\n","Updating files: 100% (59/59), done.\n","Collecting mapclassify==2.6.1 (from -r Q2_2023/requirements_colab.txt (line 1))\n","  Downloading mapclassify-2.6.1-py3-none-any.whl (38 kB)\n","Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from mapclassify==2.6.1->-r Q2_2023/requirements_colab.txt (line 1)) (3.2.1)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from mapclassify==2.6.1->-r Q2_2023/requirements_colab.txt (line 1)) (1.23.5)\n","Requirement already satisfied: pandas!=1.5.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from mapclassify==2.6.1->-r Q2_2023/requirements_colab.txt (line 1)) (1.5.3)\n","Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from mapclassify==2.6.1->-r Q2_2023/requirements_colab.txt (line 1)) (1.2.2)\n","Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from mapclassify==2.6.1->-r Q2_2023/requirements_colab.txt (line 1)) (1.11.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.5.0,>=1.4->mapclassify==2.6.1->-r Q2_2023/requirements_colab.txt (line 1)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.5.0,>=1.4->mapclassify==2.6.1->-r Q2_2023/requirements_colab.txt (line 1)) (2023.3.post1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->mapclassify==2.6.1->-r Q2_2023/requirements_colab.txt (line 1)) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->mapclassify==2.6.1->-r Q2_2023/requirements_colab.txt (line 1)) (3.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas!=1.5.0,>=1.4->mapclassify==2.6.1->-r Q2_2023/requirements_colab.txt (line 1)) (1.16.0)\n","Installing collected packages: mapclassify\n","Successfully installed mapclassify-2.6.1\n"]}],"source":["!git clone https://github.com/TPM034A/Q2_2023\n","!pip install -r Q2_2023/requirements_colab.txt\n","!mv \"/content/Q2_2023/Lab_sessions/lab_session_03/data\" /content/data\n","!mv \"/content/Q2_2023/Lab_sessions/lab_session_03/assets\" /content/assets"]},{"cell_type":"markdown","metadata":{"id":"3iLrftjQUYqK"},"source":["### `Application: Predicting perceived visual neighbourhood attractiveness` <br>\n","\n","In this lab session we will use various ML models, namely Linear regressions (LR), Random forests (RF), Multi-layer perceptrons (MLP), and Ensembles (E), to predict the **perceived visual attractiveness of neighbourhoods**. Understanding the visual attractiveness of neighbourhoods is important for various reasons. For instance, municipalities need to know which neighbourhood need attention because they are visually unattractive. Moreover, visual attractiveness of neighbourhood is important to understand and predict residential location choices, house prices, and tourist destination choices.  \n","\n","`In this lab session we aim to develop a computationally efficient ML model that is capable of mapping urban images (i.e. a Google Street view image) to visual attractiveness levels`."]},{"cell_type":"markdown","metadata":{"id":"2rk9zktfUYqK"},"source":["**Where do the true labels come from?**<br>\n","\n","Since we use supervised learning, we need to have data containing the true labels. But where do the true labels for visual attractiveness come from? The true labels come from a computer vision model that is trained on data from a so-called discrete choice experiment. In this experiment, people were placed in the hypothetical situation that they had to move to a different neighbourhood and were given two options. The visual attractiveness is learned from their choices.\n","\n","See the figure below for an example of one such choice task. More information about the survey and the model can be found in [this paper](https://arxiv.org/abs/2308.08276).\n","\n","<div>\n","<center><img src=\"assets/survey_snapshot.png\" width=\"500\"/></center>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"k_eBCgJ1UYqK"},"source":["**Why do we need a another model to (re)do the mapping?**<br>\n","\n","The mapping from image to visual attractiveness indeed already comes from a model. Therefore, one may wonder why do we need another model? Applying the computer vision model is computationally expensive. Moreover, it requires a GPU to process images in large quantities. Therefore, having a good, and **computationally efficient**, proxy model is useful. For instance, to build a map like the one below for Delft requires processing over 400k images.\n","\n","<div>\n","<center><img src=\"assets/visual_attractiveness_Delft.png\" width=\"500\"/></center>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"lYjxWIePUYqL"},"source":["**Data for this lab**<br>\n","\n","In this lab session you will work with four datasets:\n","\n","1. `data/image_tabular/image_metadata.csv`: A table csv file with image metadata (e.g., year, month or location) of Rotterdam images.\n","2. `data/image_tabular/image_embeddings.csv`: A table csv file with image embeddings of images from Rotterdam. (We will explain what image embeddings are later).\n","3. `data/geo/hexagons.gpkg`: A geospatial dataset of Rotterdam.\n","4. `data/images`: A folder with image files from Rotterdam (read below for more details).\n","\n","The first three data files are already in the `data` folder associated with this lab. The `images` folder still needs to be downloaded. The full image data set is fairly large, 14 GB, and contains 101,444 images from Rotterdam. You can download the full dataset if you want to work and explore all the images by your own, but this is not required for completing this lab session. To conduct this lab session, we have created a sub-set of the data set containing 1,000 images which is only 140 MB. This allow you to conduct the visualisations of this lab.\n","\n","The following cell will download the sub-set of images and place them in the `data` folder automatically. It can takes up to one minute to download the images. If you want to download the full dataset, just modify the the variable `download_full_dataset` to `True`."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yofdPRs6UYqL","executionInfo":{"status":"ok","timestamp":1701768815425,"user_tz":-120,"elapsed":7200,"user":{"displayName":"Irtaza Hashmi","userId":"15975256869036552807"}},"outputId":"4541b85d-dc37-4a14-e4db-5bb8de083f66"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading images...\n","Download complete!\n","Unzipping images...\n","Unzip complete!\n","Removing zip file...\n","Done!\n"]}],"source":["## IMPORTANT: You have to be on the TUDelft network (eduroam) or under eduVPN to run this script\n","download_full_dataset = False\n","\n","from assets import image_downloader as imd\n","imd.download_images(download_full_dataset)"]},{"cell_type":"markdown","metadata":{"id":"LoeN0h4wUYqL"},"source":["**Learning objectives**. After completing the following exercises you will be able to: <br>\n","1. Train multiple ML models, including `Linear regressions`, `Random Forests` and `Multi-Layer Perceptrons`\n","2. Identify the most important features and their impact on the target feature<br>\n","3. Work with embeddings of images\n","4. Explore the pros and cons of each model, and how models can complement each other to answer specific research questions <br>"]},{"cell_type":"markdown","metadata":{"id":"xP6yr46oUYqL"},"source":["#### `Organisation`\n","This lab session comprises **`3`** parts:\n","\n","1. Loading and exploring the data<br>\n","    1.1. Reading the medatada and geospatial file<br>\n","    1.2. Exploring the image metadata<br>\n","    1.3. Exploring the geospatial data<br>\n","    1.4. Visual inspection of images<br>\n","\n","2. Image embeddings<br>\n","    2.1. Embedding model<br>\n","    2.3. Exploring the embeddings<br>\n","\n","3. Training multiple models: Regression model, Random forest and MLP for predicting attractiveness<br>\n","    3.1. Preparing the dataset<br>\n","    3.2. Linear multiple regression model<br>\n","    3.4. Random Forest<br>\n","    3.5. Multilayer perceptrons<br>\n","    3.6. Comparing and reflecting on the model performances and their outcomes<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1W0xxm7UYqL"},"outputs":[],"source":["# Basic libraries\n","import numpy as np\n","import pandas as pd\n","import geopandas as gpd\n","\n","# ML tools\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n","from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, make_scorer,log_loss\n","\n","# Models\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n","from sklearn.neural_network import MLPRegressor\n","\n","# Visualization libraries\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","from mpl_toolkits.axes_grid1 import ImageGrid\n","from sklearn.tree import plot_tree\n","from branca.element import Figure\n","\n","# Other libraries\n","from pathlib import Path\n","from shapely.geometry import Point\n","from PIL import Image\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"vpttcM2hUYqL"},"source":["#### **1. Loading and exploring the data**\n","##### 1.1. Reading the medatada and geospatial data\n","\n","Before creating models, we must understand our datasets. So, first open the image metadata dataset (*data/image_tabular/image_metadata.csv*) which contains general info (metadata) about the images, and the geospatial dataset (*data/geo/hexagons.gpkg*) which contains the spatial zones we will work with."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzjayIuJUYqL"},"outputs":[],"source":["# Data folder path\n","data = Path('data')\n","\n","# Reading image df\n","img_metadata = pd.read_csv(data/'image_tabular'/'image_metadata.csv')\n","\n","# Reading hexagons gdf\n","hexagons = gpd.read_file(data/'geo'/'hexagons.gpkg')"]},{"cell_type":"markdown","metadata":{"id":"vBrYevOGUYqM"},"source":["##### 1.2. Exploring the image metadata\n","\n","The `img_metadata` DataFrame contains the metadata of thousands of Street View Images (SVI) from Rotterdam. With `head()` we can explore its different columns available."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-5H9t1TUYqM"},"outputs":[],"source":["img_metadata.head(5)"]},{"cell_type":"markdown","metadata":{"id":"jEY9bnX6UYqM"},"source":["The dataset contains:\n","\n","- **img_id**: A unique identifier of an individual image\n","- **img_path**: Image filename in the image folder\n","- **year**: Year when the picture was taken\n","- **month**: Month when the picture was taken\n","- **lat**: geospatial latitude of the image\n","- **lng**: geospatial longitude of the image\n","- **hex_id**: geospatial hexagon id where the image was taken (see more details below)\n","- **attractiveness**: Numerical value which represent the perceived attractiveness of the image\n","- **in_folder**: Binary column indicating if the image is in the sub-set of images or not"]},{"cell_type":"markdown","metadata":{"id":"6MRQbGMZUYqM"},"source":["We look at general statistics of `img_metadata` using `describe()`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwxAekUTUYqM"},"outputs":[],"source":["img_metadata.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ic5WTFFIUYqM"},"outputs":[],"source":["# Histogram of the attractiveness at image level\n","fig, ax = plt.subplots(figsize=(6, 3))\n","ax.hist(img_metadata['attractiveness'], bins=100)\n","ax.set_xlabel('Visual attractiveness')\n","ax.set_ylabel('Frequency')\n","ax.set_title('Histogram of the attractiveness at image level')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"glW2ki4bUYqM"},"source":["Here we can see:\n","- that the distribution of the attractiveness values is skewed a bit to the right. This means that there are more images with a more positive attractiveness value than negative.\n","\n","\n","Now let's briefly see how many images we have per geospatial hexagon. For this we will use the `hex_id` column. This column contains a unique identifier for each hexagon."]},{"cell_type":"markdown","metadata":{"id":"vbmPBTd1UYqM"},"source":["### <span style=\"color:skyblue\">Exercise 1: Explore the statistics and geospatial distribution of the images</span>\n","`A` First, verify how many images are available per hexagon. Visualize by table or plot. <br>\n","`B` Create histograms of images by year and month. Comment what you see. <br>\n","`C` Visualize on a map the distribution of year and month (To convert the DataFrame into a GeoDataFrame use the method provided below). Create one map for each year (2008-2022), and for each month (Jan-Dec). Interpret your results.<br>\n","`D` Do you think the month and year of the images could impact on the (predicition of) perceived visual attractiveness?<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QmcW5l4rUYqN"},"outputs":[],"source":["def dataframe_to_geodataframe_nl(original_dataframe, latitud_column_name, longitude_column_name):\n","    '''\n","    This Function converts a dataframe into a geodataframe using the latitud and longitude columns.\n","    The output will be ready to use the plot function from geopandas.\n","    '''\n","\n","    ## Creating the Point geometry using lat/lng columns\n","    original_dataframe['geometry'] = original_dataframe.apply(lambda x: Point(x[longitude_column_name], x[latitud_column_name]), axis=1)\n","\n","    ## Creating the geodataframe (we used crs 4326 because it is the code for the latitud and longitude)\n","    geodataframe = gpd.GeoDataFrame(original_dataframe, geometry='geometry', crs=4326)\n","\n","    ## Changing the crs to the same as the hexagons This is 28992, the projection used in the Netherlands.\n","    geodataframe = geodataframe.to_crs(28992)\n","\n","    return geodataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkNcFlDiUYqN"},"outputs":[],"source":["# ADD HERE YOUR ANSWER TO EXERCISE 1\n","\n","# A.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbMUeKrUUYqN"},"outputs":[],"source":["# B."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqmsumtgUYqN"},"outputs":[],"source":["# C."]},{"cell_type":"markdown","metadata":{"id":"85J17UcgUYqO"},"source":["##### 1.3. Exploring the geospatial data"]},{"cell_type":"markdown","metadata":{"id":"WCslH0aFUYqO"},"source":["The second dataset *hexagons* corresponds to a geospatial dataset of hexagons."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_l-8vEWaUYqO"},"outputs":[],"source":["hexagons.head(5)"]},{"cell_type":"markdown","metadata":{"id":"EI_Hf071UYqO"},"source":["The dataset contains only two columns. The first one, **hex_id**, it is just a unique identifier for each hexagon. The second one, **geometry**, corresponds to the coordinates of the shape and location of each hexagon. The following code, allow us to visualize each hexagon on the map using the method `explore()` from GeoPandas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-CT9dHE5UYqO"},"outputs":[],"source":["fig = Figure(width=400, height=300)\n","fig.add_child(hexagons.explore(marker_kwds={'radius': 10}, zoom_start=15))\n","display(fig)"]},{"cell_type":"markdown","metadata":{"id":"MQtfaatCUYqO"},"source":["As you can see in the map, each hexagon covers a piece of land in the city. During this lab, we will treat a hexagon as a neighbourhood. As we described before, each image from the *images* DataFrame is associated with an hexagon id (*hex_id column*), meaning that each image belongs to a neighbourhood. Now it is time to explore the (geo)relation of these two datasets."]},{"cell_type":"markdown","metadata":{"id":"6ElgIFwNUYqO"},"source":["### <span style=\"color:skyblue\">Exercise 2: Do you recongnize some geospatial patterns of the attractiveness?</span>\n","`A` Compute the average attractiveness *per hexagon*<br>\n","`B` Merge both DataFrames on the column *hex_id*<br>\n","`C` Visualise the average attractiveness of each neighbourhood, using the `explore()` method of geopandas<br>\n","`D` Do you see spatial patterns in the map? Hint: check for areas with parks, highways, and other places that you know. Does the map make intuitive sense to you?<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UDSOljXUYqO"},"outputs":[],"source":["# ADD HERE YOUR ANSWER TO EXERCISE 1\n","\n","# A.\n",""]},{"cell_type":"markdown","metadata":{"id":"Hr6o9XxDUYqO"},"source":["##### 1.4. Visual inspection of images and attractiveness\n","\n","The full image dataset contains 101,444 images. To easily visualise street images in your computer we have created a representative sample with 1,000 images located on the image folder inside data folder. Let's see an example how to visualize an image in an IPython notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-pvRq8CUYqO"},"outputs":[],"source":["image_path = lambda file: data/'images'/file\n","visual_images = img_metadata.loc[img_metadata['in_folder'] == 1].copy()"]},{"cell_type":"markdown","metadata":{"id":"TADQkr2NUYqT"},"source":["Run the following cell several times to see different images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"van0xNHnUYqT"},"outputs":[],"source":["# Create a figure\n","fig, ax = plt.subplots(1,2,figsize=(12,3.5))\n","\n","# Draw a random image\n","rnd_img = visual_images.sample(1)[['img_path','attractiveness']]\n","\n","# Visualising the random image\n","img = Image.open(image_path(rnd_img.iloc[0]['img_path'])).convert('RGB')\n","ax[0].imshow(img)\n","\n","# Remove the axis\n","# ax[0].axis('off')\n","ax[0].set_xlabel('Random image')\n","ax[0].set_xticks([])\n","ax[0].set_yticks([])\n","\n","# Add the histogram of the attractiveness of all 101k images\n","sns.histplot(img_metadata['attractiveness'],bins=50, stat='density', ax=ax[1])\n","ax[1].set_xlabel('Visual attractiveness')\n","\n","# Show where its attractiveness is located in the distribution\n","img_attractiveness = rnd_img.iloc[0]['attractiveness']\n","bin_edges = ax[1].patches\n","for patch in bin_edges:\n","    if patch.get_x() <= img_attractiveness <= patch.get_x() + patch.get_width():\n","        patch.set_facecolor('red')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Okcx_XTDUYqT"},"source":["**Reflection**<br>\n","Across the board the location in the distribution makes sense. But, some unattractive streetviews are predicted as being fairly attractive (and vice versa)."]},{"cell_type":"markdown","metadata":{"id":"lryDK_OxUYqT"},"source":["Now we go one step further, and visualise images based on its levels of attractiveness. Modify the number of percentiles (`n_percentiles`) and the number of images per percentile (`images_per_row`) to explore different groups of images.\n","\n","Images from lower percentile are less attractive than images from higher percentiles based on its attractiveness value. Take a look to this images and see if makes sense to you."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FC27rEZUYqT"},"outputs":[],"source":["# Set the number of percentiles and images per row\n","n_percentiles = 6\n","images_per_row = 5\n","\n","# Create a new column with the quantile\n","visual_images['quantile'], quantile_boundaries = pd.qcut(visual_images['attractiveness'], n_percentiles, labels=False, retbins=True)\n","\n","# Create a figure with a grid\n","fig = plt.figure(figsize=(20., 100.))\n","grid = ImageGrid(fig, 111, nrows_ncols=(n_percentiles,images_per_row+1),axes_pad=0.05)\n","\n","#\n","for i in range(n_percentiles):\n","    # Draw 'images_per_row' random images from the quantile i\n","    auxs = visual_images[visual_images['quantile']==i].sample(images_per_row)[['img_path', 'attractiveness']].reset_index(drop=True)\n","\n","    # Plot the images in the grid\n","    for j, row in auxs.iterrows():\n","        img = Image.open(image_path(row['img_path'])).convert('RGB')\n","        grid[(i*(images_per_row+1))+j+1].imshow(img)\n","        grid[(i*(images_per_row+1))+j+1].axis('off')\n","\n","    # Add the quantile label and the quantile boundaries\n","    grid[i*(images_per_row+1)].text(-300, 300, f'Quantile {i+1}\\n ({quantile_boundaries[i]:0.2f}, {quantile_boundaries[i+1]:0.2f})', fontsize=15, ha='center', va='center')\n","\n","    # Do not show axis\n","    grid[i*(images_per_row+1)].axis('off')\n","\n","# Show the figure\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"uL9U2trsUYqU"},"source":["We can inspect the tails of the distribution. That is, we can inspect the **most attractive** images and the **least attractive** images.\n","\n","Here we see the 10 least attractive images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e_nhRDUtUYqU"},"outputs":[],"source":["# Set the number of images per row\n","images_per_row = 5\n","\n","# Sort the images by attractiveness and get the first and last images\n","mins = visual_images.sort_values(by='attractiveness', ascending=True).head(images_per_row*2).reset_index(drop=True)\n","\n","# Create a figure with a grid\n","fig = plt.figure(figsize=(20., 100.))\n","grid = ImageGrid(fig, 111, nrows_ncols=(2,images_per_row),axes_pad=0.05)\n","\n","# Plot the images in the grid\n","for i, row in mins.iterrows():\n","    img = Image.open(image_path(row['img_path'])).convert('RGB')\n","    grid[i].imshow(img)\n","\n","    # Do not show axis\n","    grid[i].axis('off')\n","\n","# Show the figure\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbtFe2SPUYqU"},"outputs":[],"source":["# Set the number of images per row\n","images_per_row = 5\n","\n","# Sort the images by attractiveness and get the first and last images\n","maxs = visual_images.sort_values(by='attractiveness', ascending=True).tail(images_per_row*2).reset_index(drop=True)\n","\n","# Create a figure with a grid\n","fig = plt.figure(figsize=(20., 100.))\n","grid = ImageGrid(fig, 111, nrows_ncols=(2,images_per_row),axes_pad=0.05)\n","\n","# Plot the images in the grid\n","for i, row in maxs.iterrows():\n","    img = Image.open(image_path(row['img_path'])).convert('RGB')\n","    grid[i].imshow(img)\n","\n","    # Do not show axis\n","    grid[i].axis('off')\n","\n","# Show the figure\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Da_1jMb7UYqU"},"source":["**Question**<br>\n","Do you agree with the model's assessment of the least and most visually attractive images? Why?"]},{"cell_type":"markdown","metadata":{"id":"kaOjQbnlUYqU"},"source":["#### **2. Image embeddings**\n","##### 2.1. Embedding model\n","\n","Until now we have only visualised the images. But we would like to work with them. For that we need to transform the images into lower dimensional mathematical representations that allows for algorithmic processing. This mathematical representation is called an embedding. For instance, an image has 720 x 320 x 3 (heigh x width x color channels) pixels, while an embedding of that image typically contain only between 20 and 2,000 floating points. The figure shows conceptually shows how a classic Convolution Neural Network (CNN) produces an image embedding - which is also called **feature map**.\n","\n","\n","\n","The image embedding that we will work with is generated by the computer model that produces the visual attractiveness scores. More generally, embedding producesed by trained models are widely used for other related tasks, such as clustering, similarity search, etc.\n","\n","<div>\n","<img src=\"assets/image_embbeding.png\" width=\"500\"/>\n","</div>\n","\n","To obtain the embeddings, we need to apply the trained computer vision model to the images we are working with in this lab session. However, because this is technically beyong the scope of this course, we have already proccesed the images in advance and saved the embeddings in a csv file for you. In the following cell we load the embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXnXjANPUYqU"},"outputs":[],"source":["# Load the embeddings\n","embeddings = pd.read_csv(data/'image_tabular'/'image_embeddings.csv')"]},{"cell_type":"markdown","metadata":{"id":"yFmWIx2DUYqU"},"source":["##### 2.1. Exploring the embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAWVMnqbUYqU"},"outputs":[],"source":["embeddings.head()"]},{"cell_type":"markdown","metadata":{"id":"jEM0-Cz0UYqU"},"source":["As we can see, the embeddings have 25 elements. It means, each image is represented by 25 latent features. We don't know what these features semantically mean. But we can use them to work with the images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdcDcpXqUYqU"},"outputs":[],"source":["embeddings.describe()"]},{"cell_type":"markdown","metadata":{"id":"uZweYCPkUYqU"},"source":["### <span style=\"color:skyblue\">Exercise 3: Are year or month variables encoded by the images?</span>\n","`A` Append the year, month and attractiveness to the image embeddings columns <br>\n","`B` Create a correlation matrix between all the variables<br>\n","`C` Do you think the month, year or the attractiveness are are contained in some features of the embedding?<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6HDzqBr1UYqU"},"outputs":[],"source":["## Answer to exercise 3"]},{"cell_type":"markdown","metadata":{"id":"8rCoybY6UYqV"},"source":["#### **3. Training multiple models: Regression model, Random forest and Multilayer perceptron (MLP) for predicting attractiveness**<br>\n","Often, a researcher does not know beforehand which sort of model will do well, and which will not. Therefore, ML researchers often apply multiple models to their task and pull together their outcomes.<br>\n","\n","Next, we are going to apply 3 models:\n","1. Regression model (benchmark model)<br>\n","2. Random Forest (RF)<br>\n","3. Multilayer Perceptron (MLP)<br>\n","\n","As we will see, each of these models provide complementary insights. (maybe delete)\n","\n","##### 3.1. Preparing the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLIqYCC0UYqV"},"outputs":[],"source":["# Merging the embeddings with the original dataset\n","image_embeddings = pd.merge(img_metadata, embeddings, on='img_id')\n","image_embeddings.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-lUqmgRUYqV"},"outputs":[],"source":["# Generating the embedding columns\n","df = image_embeddings.copy()\n","emb_cols = embeddings.drop(columns=['img_id']).columns\n","features = emb_cols.to_list()\n","predict = 'attractiveness'\n","\n","# Setting all dataset between 0 and 1\n","scaler = StandardScaler()\n","df[features + [predict]] = scaler.fit_transform(df[features + [predict]])\n","\n","# Splitting the dataset\n","X_train, X_test, Y_train, Y_test = train_test_split(df[features], df[predict], test_size=0.2, random_state=44)"]},{"cell_type":"markdown","metadata":{"id":"B9IJ6dygUYqV"},"source":["Because we are going to compare different models, we create a custom evaluation function that allows us to swiftly report the following stats for the train and test data:\n","* mean square error\n","* mean absolute error\n","* R2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SceC3VQTUYqV"},"outputs":[],"source":["def eval_regression_perf(model, X_train, X_test, Y_train, Y_test):\n","\n","    # Make prediction with the trained model\n","    Y_pred_train = model.predict(X_train)\n","    Y_pred_test = model.predict(X_test)\n","\n","    # Create a function that computes the MSE, MAE, and R2\n","    def perfs(Y,Y_pred):\n","        mse = mean_squared_error(Y,Y_pred)\n","        mae = mean_absolute_error(Y,Y_pred)\n","        R2 = r2_score(Y,Y_pred)\n","        return mse,mae,R2\n","\n","    # Apply the perfs function to the train and test data sets\n","    mse_train, mae_train, r2_train = perfs(Y_train,Y_pred_train)\n","    mse_test,  mae_test , r2_test  = perfs(Y_test,Y_pred_test)\n","\n","    # Print results\n","    print('Performance')\n","    print(f'Mean Squared  Error Train | Test: \\t{mse_train:>7.4f}\\t|  {mse_test:>7.4f}')\n","    print(f'Mean Absolute Error Train | Test: \\t{mae_train:>7.4f}\\t|  {mae_test:>7.4f}')\n","    print(f'R2                  Train | Test: \\t{ r2_train:>7.4f}\\t|  {r2_test:>7.4f}\\n')"]},{"cell_type":"markdown","metadata":{"id":"KJ7fOLKcUYqV"},"source":["##### 3.2. Linear multiple regression model\n","This model usually serves as a benchmark when creating ML models. Therefore, we start with this model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kI2lhuUbUYqV"},"outputs":[],"source":["# Create and train a multiple linear regression model\n","regr = LinearRegression(fit_intercept = True)\n","\n","# Fit the model on the training data\n","regr.fit(X_train,Y_train)\n","\n","# Evaluate the performance of the trained model\n","eval_regression_perf(regr, X_train, X_test, Y_train, Y_test)\n","\n","# We take an ML approach (i.e. we use a train and tests set). But, we can still look at the regression coefficients to see which features are more important than others.\n","regr_results_int = pd.DataFrame(index = ['Intercept'], data = regr.intercept_, columns = ['Coefficient'])\n","regr_results_wei = pd.DataFrame(index = df[features].columns, data = regr.coef_, columns = ['Coefficient'])\n","regr_results = pd.concat([regr_results_int,regr_results_wei],axis=0)\n","\n","# Show the results\n","regr_results.sort_values(by='Coefficient', ascending=False).plot(kind='bar', figsize=(10, 4))"]},{"cell_type":"markdown","metadata":{"id":"1_hTLEduUYqV"},"source":["Based on these results, we can make several inferences:\n","1. The model fit relatively well (R2 of ~0.65)\n","2. The model does not seem to overfit the data: it attains the same performance on the test as on the train data sets\n","3. Based on the plot, we can infer that most feature are likely to contribute to the prediction performance, since they are nonzero.\n","4. Based on the plot, we can infer that, according to this model, Fhat004, Fhat002, and Fhat001 are the most important features."]},{"cell_type":"markdown","metadata":{"id":"PUWgd3LrUYqV"},"source":["##### 3.3. Random Forest (RF)\n","Now, we are going to try a RF on these data. RFs are known to attain a high model performance on a wide range of regression and classification tasks.\n","<br>\n","RFs need hyperparameter tuning. Instead of manually searching for the optimal hyperparameters for the RF, which is laborious, let's use sk-learn's GridSearchCV functionality to automate the hyperparameter search. In practice, typically, we want to search over the 4 most important hyperparameters: `max_depth`, `max_leaf_nodes`, `min_samples_leaf`, `max_features`. However, to avoid too long compuatational times during this lab session, here we restrict the tuning to the first two hyperparameters and try only two levels.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jG7RBT7QUYqW"},"outputs":[],"source":["# Create RF object for hyperparameter tuning\n","rf_gs = RandomForestRegressor(n_estimators=50,criterion= \"squared_error\",random_state=5)\n","\n","# Define the hyperparameter search space\n","hyperparameter_space = {\n","    'max_depth': [5, 20],\n","    'max_leaf_nodes': [500,2500],\n","    # 'min_samples_leaf': [15],     # Not tuned here too avoid long computation time\n","    # 'max_features':[0.7]          # Not tuned here too avoid long computation time\n","    }\n","\n","# Create scoring function\n","scorer = make_scorer(mean_squared_error, greater_is_better = False)\n","\n","# Create the grid_search object, with using the MLP classifier\n","folds = 5 # Number of cross validation splits\n","rf_gridsearch = GridSearchCV(rf_gs, hyperparameter_space, n_jobs=-1, cv=folds, scoring=scorer, return_train_score=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsmhGfNeUYqW"},"outputs":[],"source":["# if the model RF_gridsearch.pickle does not exist, conduct the gridsearch, else load the grid_search object\n","if not Path('RF_gridsearch.pickle').exists():\n","\n","    # Fit the grid search object to the training data\n","    # Note that this is computationally expensive!\n","    # It may take up to 5 minutes, since multiple models need to be trained multiple times\n","    rf_gridsearch.fit(X_train, Y_train)\n","\n","    # Save the trained grid_search object\n","    with open('RF_gridsearch.pickle', 'wb') as f:\n","        pickle.dump(rf_gridsearch, f)\n","else:\n","    # Load the trained grid_search object\n","    with open('RF_gridsearch.pickle', 'rb') as f:\n","        rf_gridsearch = pickle.load(f)\n","\n","# Print the best parameters\n","print('Best parameters:')\n","print(f'Tuned hyperparameters:\\n{rf_gridsearch.best_params_}')\n","print(f'Mean Squared Error:\\t{(-rf_gridsearch.best_score_):0.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_ftnVEoUYqW"},"outputs":[],"source":["# Create a dataframe from the results\n","df_results = pd.DataFrame(rf_gridsearch.cv_results_)\n","\n","# Extract relevant columns\n","df_results = df_results[['params', 'mean_test_score', 'mean_train_score']]\n","\n","# Display the top performing models in a plot\n","top_models = df_results.sort_values(by='mean_test_score', ascending=False)\n","\n","# Create a figure\n","fig, ax = plt.subplots(figsize=(10, 3))\n","\n","# Convert the dict row names to a list of strings, and replace the commas with newlines\n","param_list = [str(dict_) for dict_ in top_models['params']]\n","param_list = [param.replace(',', '\\n') for param in param_list]\n","param_list = [param.replace('{', '')   for param in param_list]\n","param_list = [param.replace('}', '')   for param in param_list]\n","\n","# Create a barplot\n","sns.barplot(x= param_list, y=top_models['mean_test_score'], ax=ax)\n","\n","# Set the x-axis labels to the parameter list\n","ax.set_xticklabels(param_list, rotation=90)\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePTj61PuUYqW"},"outputs":[],"source":["# Set the hyperparameters of the Random Forest to the best values found, e.g.\n","max_depth = rf_gridsearch.best_params_['max_depth']\n","max_leaf_nodes = rf_gridsearch.best_params_['max_leaf_nodes']\n","max_features = 0.7\n","min_samples_leaf = 15\n","\n","# Create the Random Forest object with the best hyperparameters\n","rf = RandomForestRegressor(n_estimators=100, max_depth=max_depth, max_features=max_features, max_leaf_nodes=max_leaf_nodes, min_samples_leaf=min_samples_leaf, random_state=0,n_jobs=-1)\n","\n","# Train the Randon Forest\n","rf.fit(X_train,Y_train)\n","\n","# Evaluate the performance of the hyperparameter optimised RF model\n","eval_regression_perf(rf,X_train,X_test, Y_train, Y_test)"]},{"cell_type":"markdown","metadata":{"id":"Wby56HO9UYqW"},"source":["Let's see which features are found to be most important to our Random Forest."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tbn2CQgfUYqW"},"outputs":[],"source":["# Plot the feature importances\n","fig, ax = plt.subplots(figsize=(8,10))\n","sns.barplot(y=features,x=rf.feature_importances_, ax = ax).set(title='Feature importance to predict Attractiveness')\n","ax.set_xlabel('Feature importance')\n","\n","# Print most important features\n","sorted_indices = np.argsort(rf.feature_importances_)[::-1]\n","most_imp_features_rf = [features[i] for i in sorted_indices]\n","print(f'Top 5 most important features:\\n {most_imp_features_rf[:6]}')\n","\n","# Show plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nUt_TbA7UYqW"},"source":["However, as explained before, the embeddings are not semantically interpretable.\n","\n","Nonetheless, we can draw some inference from it:\n","1. In line wit the linear multiple regression, we see that Fhat004 is the most important feature for the prediction. In contrast with the linear regression, Fhat001 is less important to the RF.\n","1. A few features are strong predictors, while most others contribute only modestly to the prediction."]},{"cell_type":"markdown","metadata":{"id":"pzoukEX0UYqW"},"source":["#### 3.4 Multilayer perceptions (MLP)"]},{"cell_type":"markdown","metadata":{"id":"Iim1GPtLUYqW"},"source":["### <span style=\"color:skyblue\">Exercise 4: Apply GridSearch now using MLP to predict the attractiveness</span>\n","More specifically, tune the size of the hidden layers, the regularisation, and the learning rate. Use for each of these hyperparameters, no more than two levels to avoid too long compuational times."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpGUu3rLUYqW"},"outputs":[],"source":["# ANSWER TO EXERCISE 4"]},{"cell_type":"markdown","metadata":{"id":"E5WpvhvZUYqW"},"source":["#### **3.5 Comparing and reflecting on the model performances and their outcomes**\n","\n","Often it helps the researcher to pull together the outcomes of multiple models. The idea of doing this is closely related to wisdom of the crowd principle. If multiple different model point towards the same conclusion, then the researcher has compounding evidence. In contrast, if different models point towards different conclusions, then the researcher knows the conclusions are weak(er) and sensitive to the choice of model.<br>\n","Let's pull together the predictions of the 3 ML models, and show them next to one another."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMuFHeGBUYqX"},"outputs":[],"source":["# Scatter true shares (x-axis) against the predicted shares by the different ML models (y-axis)\n","fig, ax = plt.subplots(1,3,figsize=(18,4), sharex=True, sharey=True)\n","ax[0].scatter(x = Y_test, y = regr.predict(X_test),  marker = '.', s = 25, alpha = 0.2, c = '#ff7f0e', label = f'Regression | R2 = {regr.score(X_test,Y_test):0.3f}')\n","ax[1].scatter(x = Y_test, y = rf.predict(X_test),    marker = '.', s = 25, alpha = 0.2, c = '#2ca02c', label = f'Random Forest| R2 = {rf.score(X_test,Y_test):0.3f}')\n","ax[2].scatter(x = Y_test, y = mlp_gs.predict(X_test),marker = '.', s = 25, alpha = 0.2, c = '#d62728', label = f'MLP | R2 = {mlp_gs.score(X_test,Y_test):0.3f}')\n","\n","# Add labels, legend, and title to each plot\n","for n in range(0,3,1):\n","    ax[n].set_xlabel('True Attractiveness')\n","    ax[n].set_ylabel('Predicted attractiveness')\n","    ax[n].set_xlim(-4,4)\n","    ax[n].set_ylim(-4,4)\n","    ax[n].legend()\n","    ax[n].set_title('True versus predicted attractiveness (test data)')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"taPFa5afUYqX"},"source":["### <span style=\"color:skyblue\">Exercise 5:  Comparison of predictions</span>\n","`A` What is the most striking difference between the predictions across the three ML models? Can you explain this?<br>\n","`B` How would the (ideal) scatter plot look like? I.e. when the model would perfectly predict our target?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_eWD-hnUYqX"},"outputs":[],"source":["# CODE YOUR ANSWERS HERE (Use as many cells as you need)\n"]},{"cell_type":"markdown","metadata":{"id":"PnVfo7sjUYqX"},"source":["Another way the compare the predictions of the three models is by creating a **kernel density plot**. A kernel density plot visualises the distribution, i.e. the probability density, of a variable, in a smooth way, see e.g. [wikipedia.org/wiki/Kernel_density_estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BS8WOYkfUYqX"},"outputs":[],"source":["# Plot kernel densities for the test data\n","fig, ax = plt.subplots(figsize=(10, 7))\n","\n","# Plot kernel density of the true distribution, i.e of Y_test\n","sns.kdeplot(data = Y_test, ax=ax, legend=True,fill=True,alpha=.3, clip =(-4,4), label = 'True')\n","\n","# Plot kernel densities of the predicted distributions, based on the three models\n","sns.kdeplot(data = regr.predict(X_test),ax=ax, legend=True,fill=True,alpha=.3, clip =(-4,4), label = f'Regression | R2 = {regr.score(X_test,Y_test):0.3f}')\n","sns.kdeplot(data = rf.predict(X_test),  ax=ax, legend=True,fill=True,alpha=.3, clip =(-4,4), label = f'Random Forest | R2 = {rf.score(X_test,Y_test):0.3f}')\n","sns.kdeplot(data = mlp_gs.predict(X_test),  ax=ax, legend=True,fill=True,alpha=.3, clip =(-4,4), label = f'MLP | R2 = {mlp_gs.score(X_test,Y_test):0.3f}')\n","\n","# Add legend and title to the plot\n","#ax.legend(['True','Regression model', 'Decision tree','Random Forest'])\n","ax.legend()\n","plt.title('True vs predicted Attractiveness')\n","ax.set_xlim(-4,4)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"voQdOcShUYqX"},"source":["### <span style=\"color:skyblue\">Exercise 6:   Conclusion and reflection on the results</span>\n","In this lab session we have analysed urban visual data, and made predictions for attractiveness using three ML models.<br>\n","`A` Based on the above analyses, which of these models can best be used to predict visual attractiveness of the urban space? Explain your answer.<br>\n","`B` In the kernel density plots, we see that the true variance of attractiveness (in blue) is considerably larger than the variance of attractiveness predicted by any of the ML models. What does this tell us about this regression problem?<br>\n","`C` How has applying multiple ML models helped you achieving your research goal?<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EqAaGabhUYqX"},"outputs":[],"source":["# CODE YOUR ANSWERS HERE (Use as many cells as you need)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.15 64-bit ('tpm34a')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"dc97a96f317a709ae2c462a7d0437fc605198aec43f9a7dadb54e6d81820938d"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}